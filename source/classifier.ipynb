{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89390774",
   "metadata": {},
   "source": [
    "# train a multinominal classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3daee",
   "metadata": {},
   "source": [
    "## load and process the data with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eded9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/chen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f24434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>— Le recueil ' e ' mentdecevéhiculesefaitexcep...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mycélium d ’ un champignon ( Claviceps purpure...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un chien , auquel on en fit avaler , succomba ...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" La trompe d ' Eustache nous amène tout natur...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>galop en me lançant des regards terribles .Le ...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             excerpt emotions\n",
       "0  — Le recueil ' e ' mentdecevéhiculesefaitexcep...  disgust\n",
       "1  Mycélium d ’ un champignon ( Claviceps purpure...  disgust\n",
       "2  Un chien , auquel on en fit avaler , succomba ...  disgust\n",
       "3  \" La trompe d ' Eustache nous amène tout natur...  disgust\n",
       "4  galop en me lançant des regards terribles .Le ...  disgust"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(text:str) -> str:\n",
    "    pattern = re.compile(r\"\\w+\\s*'\\s*\\w+\") # Pattern to match ' with optional spaces before and after\n",
    "    text = re.sub(pattern, lambda m: m.group().replace(\" \", \"\"),text)\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    french_tokenizer = PunktLanguageVars()\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "\n",
    "    tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df1 = pd.read_csv(\"../disgust.csv\")\n",
    "df2 = pd.read_csv(\"../love.csv\")\n",
    "data = pd.concat([df1,df2], ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95892233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    recueil e mentdecevéhiculesefaitexceptionnelle...\n",
       "1    mycélium champignon claviceps purpurea pousse ...\n",
       "2    chien auquel fit avaler succomba bout treize j...\n",
       "3    trompe amène tout naturellement affection cais...\n",
       "4    galop lançant regards terribles défilé pensée ...\n",
       "Name: excerpt_clean, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"excerpt_clean\"] = data['excerpt'].apply(preprocessing)\n",
    "data['emotion'] = data['emotions'].map({\"love\":0, \"sadness\":1})\n",
    "data[\"excerpt_clean\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b329fdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'experts_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/my_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'experts_clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train,X_test,y_train,y_test = train_test_split(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexperts_clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,data[\u001b[33m\"\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m\"\u001b[39m],test_size=\u001b[32m0.2\u001b[39m)\n\u001b[32m      2\u001b[39m X_train.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/my_env/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/my_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'experts_clean'"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data[\"excerpt_clean\"],data[\"emotion\"],test_size=0.2)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f4f4f",
   "metadata": {},
   "source": [
    "## Word embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5d074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(\n",
    "    X_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2\n",
    ")\n",
    "\n",
    "vocabulary = set(w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e54774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article2vec(expert, vocab, model):\n",
    "    vectors = [model.wv[word] for word in expert if word in vocab]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors,axis=0) # simply taking the mean of vectors to build the vector representation of expert\n",
    "    else:\n",
    "        return np.zeros(model.vector_size) # if non, return a vector size length vector with 0\n",
    "    \n",
    "X_train_vect = np.array([article2vec(expert,vocabulary,w2v_model ) for expert in X_train])\n",
    "X_test_vect = np.array([article2vec(expert,vocabulary,w2v_model ) for expert in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ad100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train_vect, y_train.values.ravel())\n",
    "y_pred = rf_model.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3be2df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.444 / Recall: 0.062 / Accuracy: 0.693\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8728f3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35362997658079626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.14       298\n",
      "           1       0.32      1.00      0.48       129\n",
      "\n",
      "    accuracy                           0.35       427\n",
      "   macro avg       0.66      0.54      0.31       427\n",
      "weighted avg       0.79      0.35      0.24       427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "svm = SVC(kernel=\"linear\", class_weight=\"balanced\") # without balanced, svm will prior majority class\n",
    "svm.fit(X_train_vect,y_train)\n",
    "y_pred = svm.predict(X_test_vect)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "# perfomrs better on bigger dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
